{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5407183c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "class ConstrainedKMeans:\n",
    "    def __init__(self, n_clusters, max_elements_per_cluster, max_iter=300, tol=1e-4):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_elements_per_cluster = max_elements_per_cluster\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "\n",
    "    def fit(self, X):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.centers = X[np.random.choice(n_samples, self.n_clusters, replace=False)]\n",
    "        \n",
    "        labels = np.zeros(n_samples, dtype=int)\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            # Присвоение кластеров\n",
    "            new_labels, _ = pairwise_distances_argmin_min(X, self.centers)\n",
    "\n",
    "            # Подсчет количества элементов в каждом кластере\n",
    "            cluster_counts = np.bincount(new_labels, minlength=self.n_clusters)\n",
    "\n",
    "            # Проверка на максимальное количество элементов в каждом кластере\n",
    "            for cluster_id in range(self.n_clusters):\n",
    "                if cluster_counts[cluster_id] > self.max_elements_per_cluster:\n",
    "                    # Получаем индексы элементов, принадлежащих текущему кластеру\n",
    "                    idx_to_keep = np.where(new_labels == cluster_id)[0]\n",
    "                    \n",
    "                    # Случайным образом выбираем нужное количество элементов\n",
    "                    if len(idx_to_keep) > self.max_elements_per_cluster:\n",
    "                        idx_to_remove = np.random.choice(idx_to_keep, cluster_counts[cluster_id] - self.max_elements_per_cluster, replace=False)\n",
    "                        new_labels[idx_to_remove] = -1  # Убираем элементы из этого кластера\n",
    "\n",
    "            # Переопределение меток для неподписанных экземпляров\n",
    "            unassigned_indices = np.where(new_labels == -1)[0]\n",
    "            if len(unassigned_indices) > 0:\n",
    "                nearest_clusters = self.predict(X[unassigned_indices])\n",
    "                \n",
    "                for idx, nearest in zip(unassigned_indices, nearest_clusters):\n",
    "                    if cluster_counts[nearest] < self.max_elements_per_cluster:\n",
    "                        new_labels[idx] = nearest\n",
    "                    else:\n",
    "                        # Принудительное назначение в кластер с минимальным количеством элементов\n",
    "                        least_filled_cluster = np.argmin(cluster_counts)\n",
    "                        new_labels[idx] = least_filled_cluster\n",
    "\n",
    "            # Обновление меток\n",
    "            labels = new_labels\n",
    "\n",
    "            # Пересчет центров кластеров\n",
    "            new_centers = np.array([X[labels == i].mean(axis=0) if np.any(labels == i) else self.centers[i] for i in range(self.n_clusters)])\n",
    "\n",
    "            # Проверка на сходимость\n",
    "            if np.linalg.norm(new_centers - self.centers) < self.tol:\n",
    "                break\n",
    "\n",
    "            self.centers = new_centers\n",
    "\n",
    "        # Финальная проверка меток\n",
    "        # Если все экземпляры не распределены, принудительно назначаем им метки\n",
    "        self.labels_ = labels\n",
    "        unique_labels = np.unique(self.labels_)\n",
    "        \n",
    "        if len(unique_labels) > self.n_clusters:\n",
    "            # Удаляем лишние метки\n",
    "            label_map = {}\n",
    "            new_label_count = 0\n",
    "            \n",
    "            for i in range(len(self.labels_)):\n",
    "                if self.labels_[i] not in label_map:\n",
    "                    if new_label_count < self.n_clusters:\n",
    "                        label_map[self.labels_[i]] = new_label_count\n",
    "                        new_label_count += 1\n",
    "                    else:\n",
    "                        # Если превышено количество меток, назначаем последним меткам\n",
    "                        label_map[self.labels_[i]] = new_label_count - 1\n",
    "\n",
    "            # Применяем новую нумерацию меток\n",
    "            self.labels_ = np.array([label_map[label] for label in self.labels_])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return pairwise_distances_argmin(X, self.centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2932043-05db-42c0-8cae-4ca609da2965",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\anaconda\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data: (320, 10, 246)\n",
      "Data after normalization: [[[-1.44170211e+00  7.20190967e-01 -6.77497879e-01 ...  3.83541063e-01\n",
      "   -3.12533138e-01  1.87190223e-01]\n",
      "  [-1.02350452e+00  3.90959972e-01  7.43412598e-01 ... -5.84490881e-01\n",
      "    2.04978137e+00  5.18247925e-01]\n",
      "  [-5.46366854e-01 -2.58127803e+00  4.14594983e-01 ...  3.12216153e-01\n",
      "    3.78479782e-01 -1.04720349e+00]\n",
      "  ...\n",
      "  [ 1.14514134e+00  1.27261833e-01  1.12503640e+00 ...  1.54941084e+00\n",
      "   -8.67693013e-01  9.21302912e-02]\n",
      "  [-6.98623473e-03  4.01464294e-01 -5.76699984e-02 ... -2.27019587e+00\n",
      "    7.81783352e-01 -1.54727588e+00]\n",
      "  [ 1.41313702e+00  4.04202258e-01 -8.30078044e-01 ...  3.53709513e-01\n",
      "   -9.08687734e-01 -1.69444844e+00]]\n",
      "\n",
      " [[ 9.60413616e-01  5.41853981e-03 -9.06809649e-01 ... -9.54797299e-01\n",
      "   -3.19883294e-01 -4.63113961e-01]\n",
      "  [-1.20167469e+00 -1.41383457e-01 -7.81689063e-01 ...  4.44169710e-01\n",
      "   -2.58050670e+00 -1.93454718e+00]\n",
      "  [-5.50830531e-01  3.15284323e-01  2.37690306e-01 ...  9.18433487e-01\n",
      "    2.84893277e-01  1.30118679e+00]\n",
      "  ...\n",
      "  [ 9.85689891e-01 -1.38171588e+00 -1.54152733e+00 ...  1.76172289e-01\n",
      "   -1.39876712e-03 -9.60178095e-02]\n",
      "  [-1.86061846e-02 -3.84185022e-01 -1.21272347e-01 ... -6.92618001e-02\n",
      "    3.48820456e-01  7.43213025e-01]\n",
      "  [-1.68383006e+00 -8.38085961e-01  6.31898628e-01 ...  1.45225607e+00\n",
      "    9.55297919e-01 -1.53233010e-01]]\n",
      "\n",
      " [[ 5.81150227e-01  5.65792469e-01  9.29480020e-01 ... -5.20582931e-01\n",
      "    9.43910824e-01  1.15375333e-01]\n",
      "  [-9.85162179e-01 -8.49709583e-01 -1.87312523e+00 ...  4.94608711e-01\n",
      "   -1.89533022e-01 -4.53225223e-01]\n",
      "  [ 1.05541935e+00  1.12154297e+00  1.06455189e+00 ...  3.65197151e-01\n",
      "   -1.79585730e+00 -9.16013727e-01]\n",
      "  ...\n",
      "  [-6.95345833e-01 -2.08774097e+00 -2.89005495e-01 ...  3.12087981e-01\n",
      "   -1.37295473e+00 -3.75125738e-01]\n",
      "  [-5.23998282e-01  4.61850390e-01 -5.85965918e-01 ... -3.78635939e-02\n",
      "   -2.19783674e-02  1.90348505e-01]\n",
      "  [ 1.77205870e-02  1.35743213e+00 -2.39443061e-01 ...  4.03417837e-01\n",
      "    1.53509990e+00 -9.35591228e-01]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 1.96072368e+00  7.80809221e-01 -1.70468301e+00 ...  1.00534229e+00\n",
      "    4.44920579e-01  9.11063565e-01]\n",
      "  [ 9.47621241e-02  9.65255669e-01  4.53561325e-01 ... -8.66355735e-01\n",
      "   -3.77734938e-01  1.06908791e+00]\n",
      "  [-1.40048518e-01  2.58214854e-01 -1.21523010e+00 ... -6.34003899e-02\n",
      "   -4.53660734e-01 -3.04734549e-01]\n",
      "  ...\n",
      "  [-7.00202448e-01  3.52892632e-01  5.28922967e-01 ... -9.75658955e-01\n",
      "    4.54315046e-01 -3.39124862e-01]\n",
      "  [-9.94489799e-01 -1.65672014e+00  3.81762457e-01 ...  2.22235421e+00\n",
      "   -8.28062469e-01  9.15185862e-01]\n",
      "  [-1.25865077e+00 -8.60462038e-01  9.82763600e-01 ... -1.02573616e+00\n",
      "   -1.02766539e+00 -1.35488638e+00]]\n",
      "\n",
      " [[-3.97890452e-01  2.84149889e-01  1.95137005e+00 ... -9.54797299e-01\n",
      "   -3.19883294e-01 -4.63113961e-01]\n",
      "  [-9.56982628e-02  1.66692366e+00  3.53075539e-01 ...  4.44169710e-01\n",
      "   -2.58050670e+00 -1.93454718e+00]\n",
      "  [-1.39559649e+00  9.77560084e-02 -3.32623338e-01 ...  9.18433487e-01\n",
      "    2.84893277e-01  1.30118679e+00]\n",
      "  ...\n",
      "  [-3.80462654e-01  4.84031194e-01  1.44704463e+00 ...  1.76172289e-01\n",
      "   -1.39876712e-03 -9.60178095e-02]\n",
      "  [ 1.38702674e+00  1.32899163e+00 -1.18868708e-01 ... -6.92618001e-02\n",
      "    3.48820456e-01  7.43213025e-01]\n",
      "  [ 9.12036568e-01 -2.43954233e-01 -5.06802337e-01 ...  1.45225607e+00\n",
      "    9.55297919e-01 -1.53233010e-01]]\n",
      "\n",
      " [[-8.01956422e-02  3.23338623e-01  1.64306614e+00 ... -7.76812587e-01\n",
      "   -9.61891186e-01 -1.81167356e+00]\n",
      "  [ 4.80110362e-01  3.11618404e-01  8.65752645e-01 ...  5.34587597e-02\n",
      "    9.90730731e-01  1.91047110e+00]\n",
      "  [-2.10453234e+00 -5.03521229e-01 -1.84054373e+00 ... -5.68528746e-01\n",
      "   -1.76171642e+00  5.62476209e-01]\n",
      "  ...\n",
      "  [-7.94767256e-01 -1.34323229e+00 -1.82318691e-01 ... -2.25185146e-01\n",
      "    1.01019375e+00  7.15095885e-01]\n",
      "  [ 8.72862369e-01  8.63942341e-01 -1.11085588e+00 ...  6.65137369e-01\n",
      "    3.73047435e-02 -3.54220449e-01]\n",
      "  [ 1.48742258e+00  2.03038235e+00 -3.70243965e-01 ...  1.00687255e+00\n",
      "    1.62812435e+00  8.79288557e-01]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Загрузка данных\n",
    "# Например, данные загружаются так: [320 объектов, 10 временных шагов, 246 признаков]\n",
    "data = np.load(r'../data/ts_cut/ihb.npy')\n",
    "\n",
    "# Проверка формы данных\n",
    "print(\"Shape of the data:\", data.shape)\n",
    "\n",
    "# 1. Заполнение пропусков (NaN) средними значениями по каждому признаку\n",
    "# Например, заполняем пропуски по каждому временному шагу и признаку\n",
    "data = np.nan_to_num(data, nan=np.nanmean(data, axis=0))\n",
    "\n",
    "# 2. Нормализация данных\n",
    "# Нормализуем каждый объект с учетом всех временных шагов\n",
    "scaler = StandardScaler()\n",
    "for i in range(data.shape[0]):  # для каждого объекта (320 объектов)\n",
    "    data[i] = scaler.fit_transform(data[i])\n",
    "\n",
    "# Проверка нормализованных данных\n",
    "print(\"Data after normalization:\", data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7369e2dc-6142-4b10-b43f-1e2353c7db96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of correlation matrices: (320, 246, 246)\n",
      "Shape of upper triangular matrices: (320, 30135)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Функция для расчета корреляционной матрицы\n",
    "def calculate_correlation_matrix(time_series):\n",
    "    \"\"\"\n",
    "    time_series: массив формы (10 временных шагов, 246 признаков)\n",
    "    Возвращает: корреляционная матрица формы (246, 246)\n",
    "    \"\"\"\n",
    "    # Расчет корреляционной матрицы по временным рядам для признаков\n",
    "    corr_matrix = np.corrcoef(time_series.T)  # .T для получения корреляции между признаками\n",
    "    return corr_matrix\n",
    "\n",
    "# Создание матриц корреляций для всех объектов\n",
    "correlation_matrices = []\n",
    "\n",
    "for i in range(data.shape[0]):  # 320 объектов\n",
    "    corr_matrix = calculate_correlation_matrix(data[i])\n",
    "    \n",
    "    # Заменим NaN на 0 (в случае если корреляция не может быть рассчитана для некоторых регионов)\n",
    "    corr_matrix = np.nan_to_num(corr_matrix, nan=0.0)\n",
    "    \n",
    "    # Добавляем матрицу в список\n",
    "    correlation_matrices.append(corr_matrix)\n",
    "\n",
    "# Преобразуем список в numpy массив\n",
    "correlation_matrices = np.array(correlation_matrices)\n",
    "\n",
    "# Проверим форму данных\n",
    "print(\"Shape of correlation matrices:\", correlation_matrices.shape)  # (320, 246, 246)\n",
    "\n",
    "# Дополнительно: можно извлечь верхнюю треугольную часть матрицы, чтобы уменьшить размерность\n",
    "def extract_upper_triangular(matrix):\n",
    "    \"\"\"\n",
    "    Извлекает верхнюю треугольную часть корреляционной матрицы, исключая диагональ.\n",
    "    \"\"\"\n",
    "    return matrix[np.triu_indices_from(matrix, k=1)]\n",
    "\n",
    "# Применение функции для всех матриц\n",
    "upper_triangular_matrices = np.array([extract_upper_triangular(mat) for mat in correlation_matrices])\n",
    "\n",
    "# Проверка формы результата (например, [320 объектов, количество элементов в верхнем треугольнике])\n",
    "print(\"Shape of upper triangular matrices:\", upper_triangular_matrices.shape)  # (320, число элементов в треугольнике)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b71f967-324d-4caf-8347-763fd6b43517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Agglomerative Clustering with linkage='ward'...\n",
      "Unique labels from Agglomerative Clustering with linkage='ward': [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "Submission file 'submission_agg_ward.csv' saved!\n",
      "Running Agglomerative Clustering with linkage='complete'...\n",
      "Unique labels from Agglomerative Clustering with linkage='complete': [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "Submission file 'submission_agg_complete.csv' saved!\n",
      "Running Agglomerative Clustering with linkage='average'...\n",
      "Unique labels from Agglomerative Clustering with linkage='average': [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "Submission file 'submission_agg_average.csv' saved!\n",
      "Running Agglomerative Clustering with linkage='single'...\n",
      "Unique labels from Agglomerative Clustering with linkage='single': [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "Submission file 'submission_agg_single.csv' saved!\n",
      "Total time taken for clustering: 4.02 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import time\n",
    "\n",
    "# Количество кластеров соответствует количеству субъектов\n",
    "n_clusters = 20\n",
    "\n",
    "# Методы связи для экспериментов\n",
    "linkage_methods = ['ward', 'complete', 'average', 'single']\n",
    "\n",
    "# Замер времени выполнения\n",
    "start_time = time.time()\n",
    "\n",
    "for linkage_method in linkage_methods:\n",
    "    print(f\"Running Agglomerative Clustering with linkage='{linkage_method}'...\")\n",
    "    \n",
    "    # Создаем модель Agglomerative Clustering с разными методами связи\n",
    "    agg_clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage_method)\n",
    "\n",
    "    # Обучаем модель и предсказываем метки кластеров\n",
    "    labels_agg = agg_clustering.fit_predict(upper_triangular_matrices)\n",
    "\n",
    "    # Проверим количество уникальных меток\n",
    "    unique_labels = np.unique(labels_agg)\n",
    "    print(f\"Unique labels from Agglomerative Clustering with linkage='{linkage_method}': {unique_labels}\")\n",
    "\n",
    "    # Сохраним предсказания в файл для сабмита\n",
    "    submission_agg = pd.DataFrame({'prediction': labels_agg})\n",
    "    file_name = f'submission_agg_{linkage_method}.csv'\n",
    "    submission_agg.to_csv(file_name, index=False)\n",
    "\n",
    "    print(f\"Submission file '{file_name}' saved!\")\n",
    "\n",
    "# Выводим время выполнения\n",
    "end_time = time.time()\n",
    "print(f\"Total time taken for clustering: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "794370c6-a503-48a4-b219-f00a06c4df49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file 'submission_agg_pca.csv' saved!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import pandas as pd\n",
    "\n",
    "# Количество кластеров соответствует количеству субъектов\n",
    "n_clusters = 20\n",
    "\n",
    "# Применение PCA для уменьшения размерности до 200 компонент\n",
    "pca = PCA(n_components=300)\n",
    "reduced_data = pca.fit_transform(upper_triangular_matrices)\n",
    "\n",
    "# Применение Agglomerative Clustering на данных с уменьшенной размерностью\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
    "labels_agg = agg_clustering.fit_predict(reduced_data)\n",
    "\n",
    "# Сохранение результатов в файл для сабмита\n",
    "submission_agg_pca = pd.DataFrame({'prediction': labels_agg})\n",
    "submission_agg_pca.to_csv('submission_agg_pca_300.csv', index=False)\n",
    "\n",
    "print(\"Submission file 'submission_agg_pca.csv' saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "038c77ae-d51c-4303-a70c-7dd556319a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file 'submission_agg_pca_minmax.csv' saved!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import pandas as pd\n",
    "\n",
    "# Количество кластеров соответствует количеству субъектов\n",
    "n_clusters = 20\n",
    "\n",
    "# Нормализация данных с помощью MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(upper_triangular_matrices)\n",
    "\n",
    "# Применение PCA для уменьшения размерности до 200 компонент\n",
    "pca = PCA(n_components=200)\n",
    "reduced_data = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Применение Agglomerative Clustering на данных с уменьшенной размерностью\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
    "labels_agg = agg_clustering.fit_predict(reduced_data)\n",
    "\n",
    "# Сохранение результатов в файл для сабмита\n",
    "submission_agg_pca_minmax = pd.DataFrame({'prediction': labels_agg})\n",
    "submission_agg_pca_minmax.to_csv('submission_agg_pca_minmax.csv', index=False)\n",
    "\n",
    "print(\"Submission file 'submission_agg_pca_minmax.csv' saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a488dc2f-c53d-491f-b8ab-51ff89b3ede4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file 'submission_agg_pca_selected.csv' saved!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import pandas as pd\n",
    "\n",
    "# Количество кластеров соответствует количеству субъектов\n",
    "n_clusters = 20\n",
    "\n",
    "# Отбор признаков с использованием VarianceThreshold\n",
    "selector = VarianceThreshold(threshold=0.1)  # Можно регулировать порог\n",
    "selected_data = selector.fit_transform(upper_triangular_matrices)\n",
    "\n",
    "# Применение PCA для уменьшения размерности до 200 компонент\n",
    "pca = PCA(n_components=200)\n",
    "reduced_data = pca.fit_transform(selected_data)\n",
    "\n",
    "# Применение Agglomerative Clustering на данных с уменьшенной размерностью\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
    "labels_agg = agg_clustering.fit_predict(reduced_data)\n",
    "\n",
    "# Сохранение результатов в файл для сабмита\n",
    "submission_agg_pca_selected = pd.DataFrame({'prediction': labels_agg})\n",
    "submission_agg_pca_selected.to_csv('submission_agg_pca_selected.csv', index=False)\n",
    "\n",
    "print(\"Submission file 'submission_agg_pca_selected.csv' saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "912e2e77-44a3-4dae-b2b3-97a00db7ad47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 30135)\n",
      "(320, 320)\n",
      "Submission file 'submission_agg_pca_median.csv' saved!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import pandas as pd\n",
    "\n",
    "# Количество кластеров соответствует количеству субъектов\n",
    "n_clusters = 20\n",
    "\n",
    "# Заполнение пропусков медианой\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "filled_data = imputer.fit_transform(upper_triangular_matrices)\n",
    "print(filled_data.shape)\n",
    "# Применение PCA для уменьшения размерности до 200 компонент\n",
    "pca = PCA(n_components=320)\n",
    "reduced_data = pca.fit_transform(filled_data)\n",
    "print(reduced_data.shape)\n",
    "# Применение Agglomerative Clustering на данных с уменьшенной размерностью\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
    "labels_agg = agg_clustering.fit_predict(reduced_data)\n",
    "\n",
    "# Сохранение результатов в файл для сабмита\n",
    "submission_agg_pca_median = pd.DataFrame({'prediction': labels_agg})\n",
    "submission_agg_pca_median.to_csv('submission_agg_pca_median.csv', index=False)\n",
    "\n",
    "print(\"Submission file 'submission_agg_pca_median.csv' saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfd364f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 200)\n",
      "Submission file 'submission_agg_pca_median.csv' saved!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import pandas as pd\n",
    "\n",
    "# Количество кластеров соответствует количеству субъектов\n",
    "n_clusters = 20\n",
    "\n",
    "# Заполнение пропусков медианой\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "filled_data = imputer.fit_transform(upper_triangular_matrices)\n",
    "\n",
    "# Применение PCA для уменьшения размерности до 200 компонент\n",
    "pca = PCA(n_components=200)\n",
    "reduced_data = pca.fit_transform(filled_data)\n",
    "print(reduced_data.shape)\n",
    "\n",
    "kmeans = ConstrainedKMeans(n_clusters=20, max_elements_per_cluster=16)\n",
    "kmeans.fit(reduced_data)\n",
    "kmeans.labels_\n",
    "\n",
    "# Сохранение результатов в файл для сабмита\n",
    "submission_agg_pca_median = pd.DataFrame({'prediction': kmeans.labels_})\n",
    "submission_agg_pca_median.to_csv('submission_kmeans_median.csv', index=False)\n",
    "\n",
    "print(\"Submission file 'submission_agg_pca_median.csv' saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
